---
title: "2. Plain and spatial data wrangling"
author: "Ana J. Alegre (jalegre@centrogeo.edu.mx), Cristian Silva (csilva@centrogeo.edu.mx)"
output: html_notebook
date: "11/3/2021"
---

## Introduction

*Data wrangling*, also known as *data munging* or *data cleaning* refers to the automated or manual processes by which raw data are transformed into formats more useful for analysis. These processes include identifying information gaps, filtering out irrelevant data, joining multiple data sources, or transforming their structure into a more appropriate one. 

It is possible to use the R language to read, write and manipulate data in various source formats, from plain text such as *CSV* to spatial formats such as *Shapefile*, *GeoPackage*, *TIFF* raster images, and more. One of the main advantages of using R for data management is the possibility of automating processes involving data analysis to save time.

## Objective

First, a review exercise of the main functions of the tidyverse package used for data management will be performed. For this purpose, the example dataset called `iris` (which is included in R base) will be used.

This workshop will analyze actual data provided by the Mexican government to study murders reported from December 2018 to June 2021, and comparing those that occurred in Mexico City (CDMX) with those registered in the other states of the country. 

Then, the second objective of this section is to prepare the data for the analysis to be done later in the workshop, exemplifying the use of the different data wrangling functions that R packages provide us to read and write plain and spatial data, as well as examples of how to filter, sort, group and create new columns from existing ones.

## Tidyverse review

First of all, clear all objects from the environment:
```{r Clean environment}
rm(list = ls())
```

Load `iris` sample data built-in in R:
```{r Load sample dataset}
data("iris") # This loads "iris" dataset on R environment
iris # View data
```

Load the `tidyverse` package used to manipulate the data:
```{r Load tidyverse package}
library(tidyverse)
```

### Data exploration

To quickly show data structure and contents, use the  `glimpse` function:
```{r Preview sample dataset structure}
glimpse(iris)
```

*Pipe* operator (`%>%`) takes the output from the last sentence and makes it the input for the next one. Rewrite the example below using a *pipe*:
```{r Preview sample dataset structure using pipe}
iris %>% 
  glimpse()
```

Find the unique values of the variable `Species` using `distinct` function:
```{r Distinct values}
iris %>% 
  distinct(Species) # Group unique values
```

### Select variables

Pick the `Sepal.Length` and `Species` variables using its names:
```{r Pick variables}
iris %>% 
  select(Sepal.Length, Species) %>% # Pick variables
  glimpse() # Show new data structure
```

Pick `Sepal.Length`, `Sepal.Width` and `Species` variables and multiply them to create a new variable named `Sepal.Multiply` using `mutate` function:
```{r Create new variables}
iris %>% 
  select(Sepal.Length, Sepal.Width, Species) %>% # Pick variables
  mutate(Sepal.Multiply = Sepal.Length * Sepal.Width) %>% # With the selected variables, create a new one
  glimpse() # Show data structure
```

### Filter data

Repeat operations below, but keep just the rows where `Species` value is *setosa*:
```{r Filter data}
iris %>% 
  select(Sepal.Length, Sepal.Width, Species) %>% 
  mutate(Sepal.Multiply = Sepal.Length * Sepal.Width) %>% 
  filter(Species == "setosa") # Filter rows with a condition
```

Repeat operations below, but keeping just the rows where `Species` value are *setosa* and *versicolor* and `Sepal.Length` value is greater than 4.5:
```{r Filter data with multiple conditions}
iris %>% 
  select(Sepal.Length, Sepal.Width, Species) %>% 
  mutate(Sepal.Multiply = Sepal.Length * Sepal.Width) %>%
  filter(Species %in% c("setosa", "versicolor") & Sepal.Length > 4.5) # Filter rows with multiple conditions
```

### Sort data

Repeat operations below, and sort rows by `Sepal.Length`  ascending order, using `arrange` function:
```{r Sort data}
iris %>% 
  select(Sepal.Length, Sepal.Width, Species) %>% 
  mutate(Sepal.Multiply = Sepal.Length * Sepal.Width) %>% 
  filter(Species %in% c("setosa", "versicolor") & Sepal.Length > 4.5) %>% 
  arrange(Sepal.Length) # Sort rows
```

Repeat operations below now using `Sepal.Length` in descending order then `Sepal.Multiply` in ascending order, using `arrange` function:
```{r Sort data using different ordering}
iris %>% 
  select(Sepal.Length, Sepal.Width, Species) %>% 
  mutate(Sepal.Multiply = Sepal.Length * Sepal.Width) %>% 
  filter(Species %in% c("setosa", "versicolor") & Sepal.Length > 4.5) %>% 
  arrange(desc(Sepal.Length), Sepal.Multiply) # Sort rows
```

### Grouping and summarizing data

To create categorical groups using variables, use the `group_by` function:
```{r Group data}
iris %>% 
  group_by(Species) # Group data by value
```

The dataset has no visible changes, but groups are created to perform operations on them such as:

* Number of observations (count)
* Sum
* Mean
* Minima
* Maxima
* Mean
* Median
* Standard deviation

Create new columns with statistics for the `Petal.Length` variable grouped by each value of `Species`, using `summarize` function after `group_by`:
```{r Summarize data}
iris %>% 
  group_by(Species) %>% 
  summarize(Petal.Count = n(),
            Petal.Sum = sum(Petal.Length),
            Petal.Min = min(Petal.Length),
            Petal.Max = max(Petal.Length),
            Petal.Mean = mean(Petal.Length),
            Petal.Median = median(Petal.Length),
            Petal.SD = sd(Petal.Length))
```

Another way to count the observations or rows is using `count` function after grouping. Following summarizing, the data keeps internally grouped, this is why it is often required to remove grouping and keep the dataset results (flatten) before further operations to be done, using the `ungroup` function:
```{r Ungroup data}
iris %>% 
  group_by(Species) %>% 
  count(name = "Petal.Count") %>% 
  ungroup()
```

## Reading data for analysis

In order to carry out the following exercises, we will use open data on crime incidence provided by *Secretariado Ejecutivo del Sistema Nacional de seguridad PÃºblica (SESNSP)*, publicly available on the open data portal of the Mexican government at  [https://www.datos.gob.mx/busca/dataset/incidencia-delictiva-del-fuero-comun](https://www.datos.gob.mx/busca/dataset/incidencia-delictiva-del-fuero-comun). This data aggregates crime information at the state level and will be used in the workshop to compare murder levels in CDMX with other states in the country. 

Load the `lubridate` package to easy handle date and datetime types in the dataset, and the `janitor` package to make some cleaning processes to the data:
```{r Load libraries for date handling and cleaning}
library(lubridate)
library(janitor)
```

This is a plain text dataset in comma-separated values (CSV) format. To read the file, use the function `read_csv` and assign the data to an R variable:
```{r Read data}
data_source <- "Data/IDEFC_NM.csv" # From downloaded file
# data_source <- "http://datosabiertos.segob.gob.mx/DatosAbiertos/SESNSP/IDEFC_NM__" # Directly from URL

offenses <- 
  read_csv(file = data_source,
           locale = locale(encoding = "WINDOWS-1252")) %>%  # locale parameter required in macOS or Linux, because data comes from Windows 
  clean_names() %>% # Lower case, replace whitespaces with '_' and replace special characters
  glimpse()  # Allows to preview data structure and contents
```

## Preparing data

In order to filtering by date, it is necessary to have a `month` column in date format in a *longer* table shape. To achieve this, the next chunk will execute the next operations:

1. Transform structure to *longer* format, storing each month name (in spanish) from column title in a variable named `mes_nombre` and its values in a variable named `total` using `pivot_longer` function:.
2. Get the month number from `mes_nombre` using `case_when` function to store the corresponding numeric value it on a new variable named `mes_numero`.
3. Build the month in date format using `ano` (year), `mes_numero` and the number 1 to assume the first day using the   `make_date` function to create a new variable called `mes`.
4. Finally omit the unnecessary variables `mes_nombre`, `mes_numero`, `ano` using the function `select` and the `-` preffix to exclude them in the dataset.

Execute these operations one by one linking them with *pipes* (`%>`): 
```{r Clean and reshape national data}
offenses <-
  offenses %>% 
  pivot_longer(cols = 8:19,
               names_to = "mes_nombre",
               values_to = "total") %>% 
  mutate(mes_numero = case_when(mes_nombre == "enero" ~ 1,
                                  mes_nombre == "febrero" ~ 2,
                                  mes_nombre == "marzo" ~ 3,
                                  mes_nombre == "abril" ~ 4,
                                  mes_nombre == "mayo" ~ 5,
                                  mes_nombre == "junio" ~ 6,
                                  mes_nombre == "julio" ~ 7,
                                  mes_nombre == "agosto" ~ 8,
                                  mes_nombre == "septiembre" ~ 9,
                                  mes_nombre == "octubre" ~ 10,
                                  mes_nombre == "noviembre" ~ 11,
                                  mes_nombre == "diciembre" ~ 12)) %>%
  mutate(mes = make_date(ano, mes_numero, 1)) %>% 
  select(-ano, -mes_nombre, -mes_numero) %>% 
  glimpse()
```

In order to learn the crime classification needed for the analysis, create a list of crime types based on the columns `bien_juridico_afectado` (legal asset affected), `tipo_de_delito` (crime type), `subtipo_de_delito` (crime subtype) and `modalidad` (modality) and store it as a new variable named `crime_list`:
```{r Create a crime types list}
crime_list <-
  offenses %>% 
  distinct(bien_juridico_afectado, tipo_de_delito, subtipo_de_delito, modalidad) %>% # Keep unique values of the columns
  arrange(bien_juridico_afectado, tipo_de_delito, subtipo_de_delito, modalidad) # Sort values, ascending

# Show crime categories table:
crime_list
```

Filter to keep only with the data corresponding to `Homicidio` (murder) and its subcategories, that happened from december 2018 to june 2021:
```{r Subset data}
month_begin <- make_date(2018, 12, 1)
month_end <- make_date(2020, 6, 1)

murders <-
  offenses %>% 
  filter(tipo_de_delito == "Homicidio" & between(mes, month_begin, month_end)) %>% 
  glimpse()
```

Group and summarize murder totals in a new variable `homicidios` (murders) by `clave_ent` (state ID), `entidad` (state), and `mes` (month):
```{r Grouping and summarizing data}
murders_by_state <-
  murders %>% 
  group_by(clave_ent, entidad, mes) %>% 
  summarize(homicidios = sum(total, na.rm = TRUE)) %>% 
  ungroup() %>% 
  glimpse()
```

Finally, save a copy of the processed data (will be used later on the workshop):
```{r Write data in a CSV file}
write_csv(murders_by_state, file = "Data/murders_by_state.csv")
# write_excel_csv(muders_by_state, file = "Data/murders_by_state.csv") # Use this if data will be also used in Excel
```

## Preparing spatial data

The data that will be used to study murders in CDMX are obtained from the open data portal of the City Government, available at (https://datos.cdmx.gob.mx/dataset/carpetas-de-investigacion-fgj-de-la-ciudad-de-mexico)[https://datos.cdmx.gob.mx/dataset/carpetas-de-investigacion-fgj-de-la-ciudad-de-mexico]. The records of reported crimes from this dataset are disaggregated at the level of detail and have the coordinates of the place where they occurred, which allows us to use this information to create a geographic layer of points that will be useful for the spatial analysis that will be performed later in this workshop.

R uses several add-on packages for loading, building and handling spatial data. To do so, load `sf` and `tmap` packages:
```{r Load spatial handling and visualization libraries}
library(sf) # For spatial data manipulation
library(tmap) # For creating thematic maps
```

By using `sf` package it is possible to read plain text data with geometry such as coordinates o well-known-text (WKT), and then convert it to *simple features* (spatial data) following the next instructions. First, read the plain text data using `read_csv` function:
```{r Read data to be spatialized}
sf_data_source <- "Data/carpetas_completa_julio_2021.csv.zip" # read_csv function can read compressed ZIP files

offenses_cdmx <-
  read_csv(file = sf_data_source) %>% 
  clean_names() %>% 
  glimpse()
```

Identify murder categories on data:
```{r Create a crime types list for the city}
crimes_list_cdmx <- 
  offenses_cdmx %>% 
  distinct(categoria_delito) %>% 
  arrange(categoria_delito)

crimes_list_cdmx
```

Filter murder data for the same period of time. In this dataset the variables `categoria_delito` (offense catogory) and `fecha_hechos` (event date) will be used for subsetting:
```{r Subset data for the city}
murders_cdmx <-
  offenses_cdmx %>% 
  mutate(mes = date(fecha_hechos)) %>% # Create a date column for "month" from datetime column "fecha_hechos" in order to filter
  filter(categoria_delito == "HOMICIDIO DOLOSO" & between(mes, month_begin, month_end)) %>% 
  glimpse()
```

This dataset uses a WGS84 geographic coordinate reference system (CRS), go to [https://epsg.io](https://epsg.io) for further info about EPSG codes and projections. Use the coordinates contained in columns `longitud` (longitude) and `latitud` (latitude) to build point geometries and convert dataset to *simple features* using `st_as_sf` and `st_set_crs` functions:
```{r Convert dataset to spatial}
murders_cdmx <-
  murders_cdmx %>% 
  st_as_sf(coords = c("longitud", "latitud"), # Set column names with coordinates
           na.fail = FALSE) %>% # Avoid conversion to stop if exists rows with no coordinates and leave them as empty geometries
  st_set_crs(4326) %>% # Set projection using WGS84 EPSG code 
  glimpse()
```

Once data is converted to *simple features*, it is possible use normally every data wrangling function of tidyverse on it, like `filter`, `arrange`, etc.

Quickly preview the spatialized data using function `qtm`:
```{r Preview spatialized data}
qtm(murders_cdmx) # Create a "Quick Thematic Map"
```

It is possible to write a file with the spatialized data in *ESRI Shapefile* format, but not recommended because this will not store correctly the datetime columns. Instead it is recommended use a more modern format like *GeoPackage*. Save data in a spatial format, using `sf_write` function (will be used later in the workshop):

```{r Write spatialized data in a file}
st_write(murders_cdmx, dsn = "Data/murders_cdmx.shp", delete_dsn = TRUE) # Save as Shapefile, not recommended if dataset has datetime columns
st_write(murders_cdmx, dsn = "Data/murders_cdmx.gpkg", delete_dsn = TRUE) # Save as Geopackage, recommended
```

The `st_write` function uses GDAL/OGR libraries to save data, and can write and read every file format available in their drivers. To see a list of available formats to read and write, use the `st_drivers` function:
```{r List available read and write formats}
st_drivers()
```

## References

* Wickham, H., & Grolemund, G. (2017). *R for data science: Import, tidy, transform, visualize and model data. [https://r4ds.had.co.nz](https://r4ds.had.co.nz)*. O'Reilly.
* Lovelace, R., Nowosad, J., & Muenchow, J. (2019), *Geocomputation with R. [https://geocompr.robinlovelace.net](https://geocompr.robinlovelace.net)*. CRC Press.
* Engel, C. (2019). *Using Spatial Data with R.* cengel.github.io. Retrieved September 8, 2021, from [https://cengel.github.io/R-spatial/](https://cengel.github.io/R-spatial).